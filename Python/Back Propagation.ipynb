{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Numpy library\n",
    "import numpy as np\n",
    "\n",
    "#set seed for reproducability \n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will first initialize the weights and bias needed and store them in a dictionary called W_B\n",
    "def initialize(num_f, num_h, num_out):\n",
    "    \n",
    "    '''\n",
    "    Description: This function randomly initializes the weights and biases of each layer of the neural network\n",
    "    \n",
    "    Input Arguments:\n",
    "    num_f - number of training features\n",
    "    num_h -the number of nodes in the hidden layers\n",
    "    num_out - the number of nodes in the output \n",
    "    \n",
    "    Output: \n",
    "    \n",
    "    W_B - A dictionary of the initialized parameters.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #randomly initialize weights and biases, and proceed to store in a dictionary\n",
    "    W_B = {\n",
    "        'W1': np.random.randn(num_h, num_f),\n",
    "        'b1': np.zeros((num_h, 1)),\n",
    "        'W2': np.random.randn(num_out, num_h),\n",
    "        'b2': np.zeros((num_out, 1))\n",
    "    }\n",
    "    return W_B\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will now proceed to create functions for each of our activation functions\n",
    "\n",
    "def relu (Z):\n",
    "    \n",
    "    '''\n",
    "    Description: This function performs the relu activation function on a given number or matrix. \n",
    "    \n",
    "    Input Arguments:\n",
    "    Z - matrix or integer\n",
    "    \n",
    "    Output: \n",
    "    \n",
    "   relu_Z -  matrix or integer with relu performed on it\n",
    "    \n",
    "    '''\n",
    "    relu_Z = np.maximum(Z,0)\n",
    "    \n",
    "    return relu_Z\n",
    "\n",
    "def sigmoid (Z):\n",
    "    \n",
    "    '''\n",
    "    Description: This function performs the sigmoid activation function on a given number or matrix. \n",
    "    \n",
    "    Input Arguments:\n",
    "    Z - matrix or integer\n",
    "    \n",
    "    Output: \n",
    "    \n",
    "   sigmoid_Z -  matrix or integer with sigmoid performed on it\n",
    "    \n",
    "    '''\n",
    "    sigmoid_Z = 1 / (1 + (np.exp(-Z)))\n",
    "    \n",
    "    return sigmoid_Z\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will now proceed to perform forward propagation\n",
    "\n",
    "def forward_propagation(X, W_B):    \n",
    "    '''\n",
    "    Description: This function performs the forward propagation in a vectorized form \n",
    "    \n",
    "    Input Arguments:\n",
    "    X - input training examples\n",
    "    W_B - initialized weights and biases\n",
    "    \n",
    "    Output: \n",
    "    \n",
    "   forward_results - A dictionary containing the linear and activation outputs\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Calculate the linear Z for the hidden layer\n",
    "    Z1 = np.dot(X, W_B['W1'].T)  + W_B['b1']\n",
    "    \n",
    "    #Calculate the activation ouput for the hidden layer\n",
    "    A = relu(Z1)\n",
    "    \n",
    "    #Calculate the linear Z for the output layer\n",
    "    Z2 = np.dot(A, W_B['W2'].T) + W_B['b2']\n",
    "    \n",
    "    #Calculate the activation ouput for the ouptu layer\n",
    "    Y_pred = sigmoid(Z2) \n",
    "    \n",
    "    #Save all ina dictionary \n",
    "    forward_results = {\"Z1\": Z1,\n",
    "                      \"A\": A,\n",
    "                      \"Z2\": Z2,\n",
    "                      \"Y_pred\": Y_pred}\n",
    "    \n",
    "    return forward_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will now proceed to implement the backward propagation\n",
    "\n",
    "def backward_propagation(X, W_B, Y_true):\n",
    "    '''Description: This function performs the backward propagation in a vectorized form \n",
    "    \n",
    "    Input Arguments:\n",
    "    X - input training examples\n",
    "    W_B - initialized weights and biases\n",
    "    Y_True - the true target values of the training examples\n",
    "    \n",
    "    Output: \n",
    "    \n",
    "    gradients - the calculated gradients of each parameter\n",
    "    L - the loss function\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Obtain the forward results from the forward propagation \n",
    "    \n",
    "    forward_results = forward_propagation(X, W_B)\n",
    "    Z1 = forward_results['Z1']\n",
    "    A = forward_results['A']\n",
    "    Z2 = forward_results['Z2']\n",
    "    Y_pred = forward_results['Y_pred']\n",
    "    \n",
    "    #Obtain the number of training samples    \n",
    "    no_examples = X.shape[1]\n",
    "    \n",
    "    # Calculate loss \n",
    "    L = (1/no_examples) * np.sum(-Y_true * np.log(Y_pred) - (1 - Y_true) * np.log(1 - Y_pred))\n",
    "    \n",
    "    #Calculate the gradients of each parameter needed for gradient descent \n",
    "    dLdZ2= Y_pred - Y_true\n",
    "    dLdW2 = (1/no_examples) * np.dot(dLdZ2, A.T)\n",
    "    dLdb2 = (1/no_examples) * np.sum(dLdZ2, axis=1, keepdims=True)\n",
    "    dLdZ1 = np.multiply(np.dot(W_B['W2'].T, dLdZ2), (1 - np.power(A, 2)))\n",
    "    dLdW1 = (1/no_examples) * np.dot(dLdZ1, X.T)\n",
    "    dLdb1 = (1/no_examples) * np.sum(dLdZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    #Store gradients for gradient descent in a dictionary \n",
    "    gradients = {\"dLdW1\": dLdW1,\n",
    "             \"dLdb1\": dLdb1,\n",
    "             \"dLdW2\": dLdW2,\n",
    "             \"dLdb2\": dLdb2}\n",
    "    \n",
    "    return gradients, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
